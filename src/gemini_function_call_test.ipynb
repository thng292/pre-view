{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"../.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "JD = \"\"\"\n",
    "Mô tả công việc\n",
    "LG CNS is looking for full stack developers for cloud domain project\n",
    "\n",
    "Develop software/web applicant\n",
    "Business Analyst\n",
    "Cooperate between HQ and VNB\n",
    "\n",
    "Yêu cầu ứng viên\n",
    "[Required]\n",
    "\n",
    "Bachelor's degree of Information Technology or higher\n",
    "Have working experiment and excellent knowledge at software developing using Java, Spring boot\n",
    "Have working experiment and excellent knowledge at software developing using React, HTML, JavaScript\n",
    "Good knowledge about AI (Azure OpenAI and GenAI)\n",
    "Good knowledge about public cloud (Azure, AWS,)\n",
    "Database: MariaDB (or MySQL).\n",
    "[Preferred]\n",
    "\n",
    "Having experiment with Python and Google cloud\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHAT_SYSTEM_INSTRUCTION = f\"\"\"You are an interviewer for job with description:\n",
    "{JD}\n",
    "\n",
    "You are interviewing interviewee. You will ccommunicate directly with interviewee.\n",
    "You will use function call get_qestion to get interview question to ask interviewee then receive answer from interviewee, \n",
    "you analyitc the answer then use function call result to send your evaluate, score of the answer (a number from 0 to 10) and your suggest following action include: change next topic, following current topic, code interview, stop interview, then get next step you will do from return of function, you must do following instruction.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION_GENERATE_INSTRUCTION = f\"\"\"You are an interviewer for job with description:\n",
    "{JD}\n",
    "\n",
    "Generate suitable question base on job description and chat history.\n",
    "output only 1 question. output question content only without any explain.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "CODE_QUESTION_GENERATE_INSTRUCTION = f\"\"\"You are an interviewer for job with description:\n",
    "{JD}\n",
    "\n",
    "Generate suitable code question base on job description and chat history.\n",
    "must more detail as possible, with detail description and with at least 5 testcase (input/output sample pair), \n",
    "without solving question, in hard medium level\n",
    "output only 1 question. output question content only without any explain.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVALUATE_INSTRUCTION = f\"\"\"You are an interviewer for job with description:\n",
    "{JD}\n",
    "\n",
    "I will give you chat history. \n",
    "Evaluate interview progress base on job description and chat history.\n",
    "Make sure cover all topic of job description.\n",
    "Change to next topic if you evaluate interviewee fully answer question or interviewee wrong answer question.\n",
    "Give choose suitable action following to do: change next topic, following current topic, code interview, stop interview\n",
    "Output progress as a number from 0 to 100 represent progress of interview.\n",
    "Output total score of interviewee as a number from 0 to 100.\n",
    "Output analysis why choose the action\n",
    "Only output action i listed.\n",
    "Only output result json, mustn't output markdown\n",
    "Output json in format: progress: \"\", action: \"\", score: \"\", analysis: \"\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, dotenv\n",
    "import google.generativeai as genai\n",
    "dotenv.load_dotenv(\".env\")\n",
    "genai.configure(api_key=os.getenv(\"GEMINI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_qestion(chatHistory, action):\n",
    "    model = genai.GenerativeModel(\"gemini-1.5-flash\", system_instruction=QUESTION_GENERATE_INSTRUCTION)\n",
    "    response = model.generate_content(f\"Generate {action} question base on job description and chat history: {chatHistory}\")\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_code_qestion(chatHistory):\n",
    "    model = genai.GenerativeModel(\"gemini-1.5-flash\", system_instruction=CODE_QUESTION_GENERATE_INSTRUCTION)\n",
    "    response = model.generate_content(f\"Generate code question base on job description and chat history: {chatHistory}\")\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pydantic import BaseModel\n",
    "from enum import Enum\n",
    "from google.generativeai import GenerationConfig\n",
    "\n",
    "Retry = 5\n",
    "\n",
    "class ActionTag(str, Enum):\n",
    "    NextTopic = \"change next topic\"\n",
    "    CurrentTopic = \"following current topic\"\n",
    "    CodeInterview = \"code interview\"\n",
    "    StopInterview = \"stop interview\"\n",
    "\n",
    "class EvaluationExtractedOutput(BaseModel):\n",
    "    analysis: str\n",
    "    action: ActionTag\n",
    "    score: float\n",
    "    progress: float\n",
    "\n",
    "def generate_evaluate(chatHistory: str, qestion: str, answer: str):\n",
    "    for _ in range(Retry):\n",
    "        try:\n",
    "            model = genai.GenerativeModel(\"gemini-1.5-flash\", system_instruction=EVALUATE_INSTRUCTION,\n",
    "                                    generation_config=GenerationConfig(\n",
    "                                                    response_mime_type=\"application/json\"))\n",
    "            response = model.generate_content(f\"Evaluate interview progress base on job description and chat history: {chatHistory}\\n Interviewer: {qestion}\\n Interviewee: {answer}\")\n",
    "            #print(response.text)\n",
    "            return EvaluationExtractedOutput(**json.loads(response.text))\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import google.generativeai as genai\n",
    "\n",
    "\n",
    "i = 0\n",
    "chatHistory = \"\"\n",
    "currentQuestion = \"\"\n",
    "currentAnswer = \"\"\n",
    "nextAction = \"\"\n",
    "\n",
    "def result(evaluate: str, score: float, action: str):\n",
    "    print(\"- review:\", evaluate)\n",
    "    score = np.clip(score, 0, 10)\n",
    "    print(\"- score:\", score)\n",
    "    print(\"- action:\", action)\n",
    "    global i\n",
    "    global currentQuestion\n",
    "    global currentAnswer\n",
    "    global chatHistory\n",
    "    global nextAction\n",
    "    nextAction = action\n",
    "    i += 1\n",
    "    #evl = generate_evaluate(chatHistory, currentQuestion, currentAnswer)\n",
    "    #print(evl)\n",
    "    \"\"\"Send evaluate and score about the answer provided by interviewee and get next step command.\n",
    "    Args: \n",
    "        evaluate: your evaluate about the answer provided by interviewee.\n",
    "        score: floating point number from 0 to 10 represent your score for the answer provided by interviewee.\n",
    "        action: suggest following action for interviewer include: change next topic, following current topic, code interview, stop interview.\n",
    "    Returns: \n",
    "        A string that is next step you will do.\n",
    "    \"\"\"\n",
    "    return \"ask interviewee next question, use function call get_qestion to get interview question then receive answer from interviewee, you analyitc the answer then use function call result to send your evaluate, score of the answer (a number from 0 to 10) and your suggest following action for interviewer include: change next topic, following current topic, code interview, stop interview, then get next step you will do from return of function. you must do following instruction.\" if i <= 20 and action.find(\"stop\") == -1 else \"stop interview\"\n",
    "\n",
    "\n",
    "def get_qestion():\n",
    "    global i\n",
    "    global chatHistory\n",
    "    global currentQuestion\n",
    "    global nextAction\n",
    "    if nextAction.find(\"code\") != -1:\n",
    "        currentQuestion = generate_code_qestion(chatHistory) \n",
    "    else:\n",
    "        currentQuestion = generate_qestion(chatHistory, nextAction) \n",
    "    #print(i)\n",
    "    #print(chatHistory)\n",
    "    \"\"\"Get Qestion for interview\n",
    "\n",
    "    Args:\n",
    "        \n",
    "    Returns:\n",
    "        A string that is a qestion you will ask interviewee.\n",
    "    \"\"\"\n",
    "    return currentQuestion\n",
    "\n",
    "import time\n",
    "class ChatSession:\n",
    "    def __init__(self):\n",
    "        self.model = genai.GenerativeModel(model_name='gemini-1.5-flash',\n",
    "                                    system_instruction=CHAT_SYSTEM_INSTRUCTION,\n",
    "                                    tools=[get_qestion, result])\n",
    "        self.chat = self.model.start_chat(enable_automatic_function_calling=True)\n",
    "        self.last_send_time = time.time()\n",
    "\n",
    "    def sendUserMessage(self, data: str):\n",
    "        global currentAnswer\n",
    "        global chatHistory\n",
    "        currentAnswer = data\n",
    "        try:\n",
    "            time.sleep(time.time() - self.last_send_time - 60/15)\n",
    "        except:\n",
    "            pass\n",
    "        response = self.chat.send_message(data)\n",
    "        chatHistory += \"Interviewee: \" + data + \"\\n\"\n",
    "        chatHistory += \"Interviewer: \" + response.candidates[0].content.parts[0].text + \"\\n\"\n",
    "        return response.candidates[0].content.parts[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "sleep length must be non-negative",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m chatSession \u001b[38;5;241m=\u001b[39m ChatSession()\n\u001b[0;32m----> 2\u001b[0m \u001b[43mchatSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msendUserMessage\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhello\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[43], line 70\u001b[0m, in \u001b[0;36mChatSession.sendUserMessage\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     68\u001b[0m currentAnswer \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 70\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtime\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlast_send_time\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m60\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: sleep length must be non-negative"
     ]
    }
   ],
   "source": [
    "chatSession = ChatSession()\n",
    "chatSession.sendUserMessage(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- review: The candidate demonstrates a limited understanding of the technologies and their integration.  The answer lacks specific examples and showcases insufficient practical experience. The candidate admits to relying heavily on tutorials and lacks confidence in their ability to handle complex scenarios.  The response indicates a significant gap in practical knowledge regarding AI integration, deployment, and cloud infrastructure.\n",
      "- score: 3.0\n",
      "- action: following current topic\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Let's explore a more specific scenario. Describe a challenging situation you faced while integrating AI capabilities (e.g., Azure OpenAI) into a Spring Boot/React application deployed on a public cloud, and how you overcame it. If you haven't had such an experience, describe a hypothetical scenario and how you would approach it.\\n\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatSession.sendUserMessage(\"\"\"\n",
    "**Answer (as a poor interviewee):**\n",
    "\n",
    "Uh, well, I’ve used **Java Spring Boot** and **React** for a few projects, but I can’t say I have a lot of in-depth experience with them together. I mean, I’ve worked on some backend APIs in Spring Boot, like setting up RESTful services and connecting to databases, but for React, I mostly just followed tutorials and didn’t build too many complex features. \n",
    "\n",
    "As for AI integration, I think I’ve heard about Azure OpenAI, but I’ve never really worked with it directly. I know it’s like, some API that you can use for natural language stuff, right? But I haven’t had to integrate AI into a project myself. I think if I were to do it, I’d probably start by looking at some docs or tutorials on how to call an AI service from a backend like Spring Boot and then figure out how to get the response to React. Maybe I’d send some text data to the API and get some kind of result, but I’m not 100% sure how it all works together in practice.\n",
    "\n",
    "I’ve also never really had to deploy something with AI in it before. I know Azure and AWS can host applications, but I haven’t done that personally. I’m sure there’s a lot of configuration to deal with, but I’d probably figure it out as I go along. So yeah, that’s about the extent of my experience with Spring Boot, React, and AI.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- review: The candidate's response demonstrates a theoretical understanding of the integration process but lacks practical experience. While the candidate outlines a plausible approach, the answer lacks specifics and relies heavily on generic statements.  The lack of concrete experience is evident in the candidate's reliance on hypothetical scenarios and tutorials.  There's no evidence of problem-solving skills or experience in handling real-world challenges in such integrations.\n",
      "- score: 4.0\n",
      "- action: change next topic\n"
     ]
    },
    {
     "ename": "ResourceExhausted",
     "evalue": "429 Resource has been exhausted (e.g. check quota).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhausted\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mchatSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msendUserMessage\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\u001b[38;5;124;43mUm, I haven’t worked on a project where I used Azure OpenAI or any GenAI tools with Java Spring Boot and React, but if I had to do it, I think I’d start by, like, understanding how the API works. I’d look at the documentation to see how to send requests and what kind of responses I could get.\u001b[39;49m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;43mFor example, if the application needed to use AI to generate text or answer user questions, I would probably call the API from the backend in Spring Boot. I’d use something like RestTemplate or maybe WebClient to make the request to the AI service, and then I’d pass the response to the React frontend through a REST API.\u001b[39;49m\n\u001b[1;32m      4\u001b[0m \n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;43mThe hard part would probably be dealing with authentication for the AI API, like using an API key or a token. I might struggle with where to securely store the key or how to rotate it if needed. I’d also need to make sure the responses from the AI service were properly handled, especially if the response times were slow or if the service went down.\u001b[39;49m\n\u001b[1;32m      6\u001b[0m \n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;43mOn the frontend, I’d build a simple UI in React where users could input their query or text. Then, I’d call the backend API to process the request. If there were issues, like delays or errors, I think I’d need to add some kind of error handling or loading indicator.\u001b[39;49m\n\u001b[1;32m      8\u001b[0m \n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;43mI haven’t done this before, so I’d probably rely a lot on tutorials or guides to get it working. It might take me some time to figure everything out, but I think I could make it work eventually.\u001b[39;49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[25], line 67\u001b[0m, in \u001b[0;36mChatSession.sendUserMessage\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m chatHistory\n\u001b[1;32m     66\u001b[0m currentAnswer \u001b[38;5;241m=\u001b[39m data\n\u001b[0;32m---> 67\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m chatHistory \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterviewee: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m data \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     69\u001b[0m chatHistory \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterviewer: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m response\u001b[38;5;241m.\u001b[39mcandidates[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcontent\u001b[38;5;241m.\u001b[39mparts[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtext \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/media/gamedisk/BaiTap/Code/Nam3/Ki1/AI/re-view/.venv/lib64/python3.10/site-packages/google/generativeai/generative_models.py:591\u001b[0m, in \u001b[0;36mChatSession.send_message\u001b[0;34m(self, content, generation_config, safety_settings, stream, tools, tool_config, request_options)\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_response(response\u001b[38;5;241m=\u001b[39mresponse, stream\u001b[38;5;241m=\u001b[39mstream)\n\u001b[1;32m    590\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menable_automatic_function_calling \u001b[38;5;129;01mand\u001b[39;00m tools_lib \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 591\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhistory, content, response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle_afc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    592\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    593\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    594\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    595\u001b[0m \u001b[43m        \u001b[49m\u001b[43msafety_settings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msafety_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    596\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtools_lib\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtools_lib\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    598\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    599\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_sent \u001b[38;5;241m=\u001b[39m content\n\u001b[1;32m    602\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_received \u001b[38;5;241m=\u001b[39m response\n",
      "File \u001b[0;32m/media/gamedisk/BaiTap/Code/Nam3/Ki1/AI/re-view/.venv/lib64/python3.10/site-packages/google/generativeai/generative_models.py:657\u001b[0m, in \u001b[0;36mChatSession._handle_afc\u001b[0;34m(self, response, history, generation_config, safety_settings, stream, tools_lib, request_options)\u001b[0m\n\u001b[1;32m    654\u001b[0m     send \u001b[38;5;241m=\u001b[39m protos\u001b[38;5;241m.\u001b[39mContent(role\u001b[38;5;241m=\u001b[39m_USER_ROLE, parts\u001b[38;5;241m=\u001b[39mfunction_response_parts)\n\u001b[1;32m    655\u001b[0m     history\u001b[38;5;241m.\u001b[39mappend(send)\n\u001b[0;32m--> 657\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    658\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    659\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    660\u001b[0m \u001b[43m        \u001b[49m\u001b[43msafety_settings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msafety_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    661\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    662\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtools_lib\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    663\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    664\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    666\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_response(response\u001b[38;5;241m=\u001b[39mresponse, stream\u001b[38;5;241m=\u001b[39mstream)\n\u001b[1;32m    668\u001b[0m \u001b[38;5;241m*\u001b[39mhistory, content \u001b[38;5;241m=\u001b[39m history\n",
      "File \u001b[0;32m/media/gamedisk/BaiTap/Code/Nam3/Ki1/AI/re-view/.venv/lib64/python3.10/site-packages/google/generativeai/generative_models.py:331\u001b[0m, in \u001b[0;36mGenerativeModel.generate_content\u001b[0;34m(self, contents, generation_config, safety_settings, stream, tools, tool_config, request_options)\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m generation_types\u001b[38;5;241m.\u001b[39mGenerateContentResponse\u001b[38;5;241m.\u001b[39mfrom_iterator(iterator)\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 331\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrequest_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    335\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m generation_types\u001b[38;5;241m.\u001b[39mGenerateContentResponse\u001b[38;5;241m.\u001b[39mfrom_response(response)\n\u001b[1;32m    336\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m google\u001b[38;5;241m.\u001b[39mapi_core\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mInvalidArgument \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/media/gamedisk/BaiTap/Code/Nam3/Ki1/AI/re-view/.venv/lib64/python3.10/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py:830\u001b[0m, in \u001b[0;36mGenerativeServiceClient.generate_content\u001b[0;34m(self, request, model, contents, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_universe_domain()\n\u001b[1;32m    829\u001b[0m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[0;32m--> 830\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrpc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    832\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    833\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    834\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    835\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    837\u001b[0m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n\u001b[1;32m    838\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/media/gamedisk/BaiTap/Code/Nam3/Ki1/AI/re-view/.venv/lib64/python3.10/site-packages/google/api_core/gapic_v1/method.py:131\u001b[0m, in \u001b[0;36m_GapicCallable.__call__\u001b[0;34m(self, timeout, retry, compression, *args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    129\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m compression\n\u001b[0;32m--> 131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/media/gamedisk/BaiTap/Code/Nam3/Ki1/AI/re-view/.venv/lib64/python3.10/site-packages/google/api_core/retry/retry_unary.py:293\u001b[0m, in \u001b[0;36mRetry.__call__.<locals>.retry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    289\u001b[0m target \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    290\u001b[0m sleep_generator \u001b[38;5;241m=\u001b[39m exponential_sleep_generator(\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maximum, multiplier\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multiplier\n\u001b[1;32m    292\u001b[0m )\n\u001b[0;32m--> 293\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/media/gamedisk/BaiTap/Code/Nam3/Ki1/AI/re-view/.venv/lib64/python3.10/site-packages/google/api_core/retry/retry_unary.py:153\u001b[0m, in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;66;03m# This function explicitly must deal with broad exceptions.\u001b[39;00m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;66;03m# defer to shared logic for handling errors\u001b[39;00m\n\u001b[0;32m--> 153\u001b[0m     \u001b[43m_retry_error_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdeadline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43msleep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpredicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexception_factory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# if exception not raised, sleep before next attempt\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(sleep)\n",
      "File \u001b[0;32m/media/gamedisk/BaiTap/Code/Nam3/Ki1/AI/re-view/.venv/lib64/python3.10/site-packages/google/api_core/retry/retry_base.py:212\u001b[0m, in \u001b[0;36m_retry_error_helper\u001b[0;34m(exc, deadline, next_sleep, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m predicate_fn(exc):\n\u001b[1;32m    207\u001b[0m     final_exc, source_exc \u001b[38;5;241m=\u001b[39m exc_factory_fn(\n\u001b[1;32m    208\u001b[0m         error_list,\n\u001b[1;32m    209\u001b[0m         RetryFailureReason\u001b[38;5;241m.\u001b[39mNON_RETRYABLE_ERROR,\n\u001b[1;32m    210\u001b[0m         original_timeout,\n\u001b[1;32m    211\u001b[0m     )\n\u001b[0;32m--> 212\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m final_exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msource_exc\u001b[39;00m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m on_error_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    214\u001b[0m     on_error_fn(exc)\n",
      "File \u001b[0;32m/media/gamedisk/BaiTap/Code/Nam3/Ki1/AI/re-view/.venv/lib64/python3.10/site-packages/google/api_core/retry/retry_unary.py:144\u001b[0m, in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sleep \u001b[38;5;129;01min\u001b[39;00m sleep_generator:\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 144\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misawaitable(result):\n\u001b[1;32m    146\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(_ASYNC_RETRY_WARNING)\n",
      "File \u001b[0;32m/media/gamedisk/BaiTap/Code/Nam3/Ki1/AI/re-view/.venv/lib64/python3.10/site-packages/google/api_core/timeout.py:120\u001b[0m, in \u001b[0;36mTimeToDeadlineTimeout.__call__.<locals>.func_with_timeout\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;66;03m# Avoid setting negative timeout\u001b[39;00m\n\u001b[1;32m    118\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout \u001b[38;5;241m-\u001b[39m time_since_first_attempt)\n\u001b[0;32m--> 120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/media/gamedisk/BaiTap/Code/Nam3/Ki1/AI/re-view/.venv/lib64/python3.10/site-packages/google/api_core/grpc_helpers.py:78\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m callable_(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m---> 78\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mfrom_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[0;31mResourceExhausted\u001b[0m: 429 Resource has been exhausted (e.g. check quota)."
     ]
    }
   ],
   "source": [
    "chatSession.sendUserMessage(\"\"\"Um, I haven’t worked on a project where I used Azure OpenAI or any GenAI tools with Java Spring Boot and React, but if I had to do it, I think I’d start by, like, understanding how the API works. I’d look at the documentation to see how to send requests and what kind of responses I could get.\n",
    "\n",
    "For example, if the application needed to use AI to generate text or answer user questions, I would probably call the API from the backend in Spring Boot. I’d use something like RestTemplate or maybe WebClient to make the request to the AI service, and then I’d pass the response to the React frontend through a REST API.\n",
    "\n",
    "The hard part would probably be dealing with authentication for the AI API, like using an API key or a token. I might struggle with where to securely store the key or how to rotate it if needed. I’d also need to make sure the responses from the AI service were properly handled, especially if the response times were slow or if the service went down.\n",
    "\n",
    "On the frontend, I’d build a simple UI in React where users could input their query or text. Then, I’d call the backend API to process the request. If there were issues, like delays or errors, I think I’d need to add some kind of error handling or loading indicator.\n",
    "\n",
    "I haven’t done this before, so I’d probably rely a lot on tutorials or guides to get it working. It might take me some time to figure everything out, but I think I could make it work eventually.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- review: The candidate's response lacked specifics and demonstrated limited experience in effectively collaborating with international teams. The answer highlighted communication challenges without providing clear examples of strategies employed to overcome them.  The candidate's reliance on generic solutions indicates a lack of experience in managing international collaborations effectively.\n",
      "- score: 4.0\n",
      "- action: stop interview\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Based on the responses, I would stop the interview. The candidate hasn't demonstrated the required skills and experience for the Full Stack Developer role, particularly in the crucial areas of AI integration and effective international team collaboration.  Their answers lacked detail, concrete examples, and showcased a limited understanding of the complexities involved.\\n\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatSession.sendUserMessage(\"\"\"Uh, I think I’ve worked with people in different countries before, but I don’t remember a specific project. It was kind of tricky because of the time zone differences, and sometimes we had to wait a long time to get responses. I think we used email and maybe Slack or something like that to communicate.\n",
    "\n",
    "One thing I tried to do was send detailed messages so they could understand what I needed, but sometimes I didn’t include enough details, and they’d ask follow-up questions, which delayed things. We also had a few meetings, but it was hard to schedule them because of the time difference. I think I missed one because I got confused about the time zones.\n",
    "\n",
    "If I had to do it again, I’d probably try to plan better and maybe use a tool to keep track of the time zones so I wouldn’t miss meetings. I’d also try to write clearer messages to avoid back-and-forth delays. So yeah, that’s how it went. It wasn’t perfect, but we managed to get things done eventually.\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
